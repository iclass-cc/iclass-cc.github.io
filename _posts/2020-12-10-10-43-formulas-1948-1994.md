---
mathjax: true
key: 2020-12-10-10-43-formulas-1948-1994
title: Formulas 数学公式 1948-1994
date: 2020-12-10 10:43 +0800
tags: Formulas 数学公式
---

$$H\left(S\right) \leqslant m\left(S\right) \leqslant H\left(S\right) + 1$$

$${ Entropy, } \ H(X)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log _{2}\left(P\left(x_{i}\right)\right)$$

The mathematician Claude Shannon introduced the entropy in information theory in 1948. Entropy in information theory can be defined as the expected number of bits of information contained in an event. For instance, tossing a fair coin has the entropy of 1. It is because of the probability of having a head or tail is 0.5. The amount of information required to identify it’s head or tail is one by asking one, yes or no question — “is it head ? or is it tail?”. If the entropy is higher, that means we need more information to represent an event. Now, we can say that entropy increases with increases in uncertainty. Another example is that crossing the street has less number of information required to represent/ store/ communicate than playing a poker game.

<!--more-->
