---
mathjax: true
key: 2020-12-10-10-43-formulas-1948-1994
title: Formulas 数学公式 1948-1994
date: 2020-12-10 10:43 +0800
tags: Formulas 数学公式
---

## 1948

$$H(S) \leqslant m(S) \leqslant H(S) + 1$$

The mathematician Claude Shannon introduced the entropy in information theory in 1948. **Entropy** in information theory can be defined as the expected number of bits of information contained in an event. For instance, tossing a fair coin has the entropy of 1. It is because of the probability of having a head or tail is 0.5. The amount of information required to identify it’s head or tail is one by asking one, yes or no question — “is it head ? or is it tail?”. If the entropy is higher, that means we need more information to represent an event. Now, we can say that entropy increases with increases in uncertainty. Another example is that crossing the street has less number of information required to represent / store / communicate than playing a poker game.

Calculating how much information is in a random variable, X={x0, x1, …. , xn} is same as calculating the information for the probability distribution of the events in the random variable. In that sense, entropy is considered as average bits of information required to represent an event drawn from the probability distribution. Entropy for a random variable X can be computed using the below equation:

$${ Entropy, } \ H(X)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log _{2}\left(P\left(x_{i}\right)\right)$$

## 1971

$$P \stackrel{?}{=} N P$$

In 1971, **Stephen Cook** introduced the precise statement of the P versus NP problem in his article "The complexity of theorem proving procedures". Today, many people consider this problem to be the most important open problem in computer science.

## 1976

$$x_{n+1}=r x_{n}\left(1-x_{n}\right)$$

 The **logistic map** – also known as the “logistic difference equation” – was made famous by **Robert May** in 1976 when he used it to model the behaviour of each generation of a biological species, and found to his amazement that from this simplest of mathematical models, complex dynamics can emerge.

The logistic map is a discrete recursive mathematical function that maps the output of one iteration of the function onto the input of the next.  Thus the logistic map is a simple mathematical way of examining the effects of feedback on population growth.

## 1994

$$\begin{array}{c}
x^{n}+y^{n}=2^{n} \quad n>2 \\
=> \\
x y z=0
\end{array}$$

In 1994, **Andrew Wiles**, 62, cracked Fermat's Last Theorem, which was put forth by 17th-century mathematician Pierre de Fermat.

<!--more-->
